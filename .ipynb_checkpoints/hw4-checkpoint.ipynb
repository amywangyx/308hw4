{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import unicodedata\n",
    "from elasticsearch import helpers\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import ast\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet \n",
    "from collections import Counter\n",
    "import spacy\n",
    "from nltk import ne_chunk,pos_tag\n",
    "from nltk.tree import Tree\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Term Matrix\n",
    "Previously method used in hw3 is not doable because now we are stripping each news as an item instead of each sentences. \n",
    "\n",
    "Don't run it over and over again after initial run- not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:\\\\Users\\\\Amy\\\\Downloads\\\\all1314text\\\\*.txt'\n",
    "files=glob.glob(path)\n",
    "text=''\n",
    "news=[]\n",
    "for document in files:\n",
    "    with open(document,'r',errors='ignore') as single_file:\n",
    "        articles=[]\n",
    "        for eachline in single_file:\n",
    "            #eachline is a sentence\n",
    "            newsart = unicodedata.normalize('NFKD', eachline).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            articles.append(newsart)\n",
    "    news.append(articles)\n",
    "    if i%50==0:\n",
    "        print (f\"Read {document.name}\")\n",
    "    \n",
    "    \n",
    "    # Write to a file\n",
    "file=open('C:\\\\Users\\\\Amy\\\\Downloads\\\\all1314text\\\\documentmatrix.txt','w')\n",
    "file.write(str(news))\n",
    "file.close()\n",
    "#with open(outfname, \"w\") as outf:\n",
    "#    outf.write(str(news))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:\\\\Users\\\\Amy\\\\Downloads\\\\all1314text\\\\documentmatrix.txt\", \"r\") as document:\n",
    "    news = ast.literal_eval(document.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"UPDATE: As the Fiscal Cliff bill gets closer to likely passage, the Aussie market (one of the few markets that's trading right now) is at its highs of the day.  Hong Kong is up over 1.2%. EARLIER: There are very few markets open (Japan/China on Holiday, futures still closed) but one that's open is Australia. And so far the Washington theatrics are not a problem. \\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35898, [])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "actions=[]\n",
    "for dayindex,daynews in enumerate(news):\n",
    "    for articleindex,article in enumerate(daynews):\n",
    "        \n",
    "    \n",
    "        action = {\n",
    "        \"_index\": \"article_index\",\n",
    "        \"_type\": \"eachnews\",\n",
    "        \"_id\": f\"{dayindex}-{articleindex}\",\n",
    "        \"body\":str(article)}\n",
    "        actions.append(action)\n",
    "#print (actions[:5])\n",
    "# \"body\": \" \".join(article)} \n",
    "helpers.bulk(es, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test strings used later on\n",
    "questionstring1= 'what % of drop in gdp is caused by investment'\n",
    "questionstring2='who is the ceo of company Tesla'\n",
    "questionstring3='who is the ceo of company Google'\n",
    "questionstring4 = 'what impacts GDP'\n",
    "questionstring5='which comapany went bankrupt in July of 2013?'\n",
    "questionstring6=\"Which company went bankrupt in September 2008?\"\n",
    "questionstring7=\"which company declared bankruptcy in April 2014?\"\n",
    "questionstring8= 'what percentage of drop in gdp is associated with tax'\n",
    "questionstring9= 'what percent of increase in gdp is associated with oil price'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Classifier\n",
    "we have two approaches to determine the question type, one is rule-based approach, another is automated classifer. Since rule-based approach is less tolerant with different wordings, I chose to compute the tf-idf score for each question as a term vector, and then get cosine similarity between this question vector with four questions and pick the highest similarity one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to classify questions\n",
    "def classify_questions(questionstring):\n",
    "    questioninreal = questionstring\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    #questioninreal = questioninreal.lower()\n",
    "    tokenizedquestion=nltk.word_tokenize(questioninreal)\n",
    "\n",
    "\n",
    "    question1 = \"which companies went bankrupt in month x of year y?\"\n",
    "    question2 = \"what affects gdp?\"\n",
    "    question3 = \"what percentage % of drop or increase change in gdp is associated with results from this property consumption consumer spending government investment imports exports foreign trade?\"\n",
    "    question4 = \"who is the ceo of company x?\"\n",
    "\n",
    "    questionlist=[]\n",
    "    questionlist.append(question1)\n",
    "    questionlist.append(question2)\n",
    "    questionlist.append(question3)\n",
    "    questionlist.append(question4)\n",
    "    consinesim_list=[]\n",
    "    for i in range(len(questionlist)):\n",
    "        response=tfidf_vectorizer.fit_transform([questioninreal,questionlist[i]])\n",
    "        cosin_sim=cosine_similarity(response)\n",
    "        consinesim_list.append(cosin_sim)\n",
    "\n",
    "    similaritylist=[]\n",
    "    for item in consinesim_list:\n",
    "\n",
    "        temporarymin=min(item[0])\n",
    "        similaritylist.append(temporarymin)\n",
    "    typeq= max(similaritylist)\n",
    "    index=np.argmax(similaritylist)+1 #index in python is different\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "4\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(classify_questions(questionstring1))\n",
    "print(classify_questions(questionstring2))\n",
    "print(classify_questions(questionstring3))\n",
    "print(classify_questions(questionstring4))\n",
    "print(classify_questions(questionstring5))\n",
    "print(classify_questions(questionstring6))\n",
    "print(classify_questions(questionstring7))\n",
    "print(classify_questions(questionstring8))\n",
    "print(classify_questions(questionstring9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract keywords from the questions and formulate queries for es\n",
    "get rid of stopwords\n",
    "twist towards specific types of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words_general = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processq(questionstring):\n",
    "    newq=[]\n",
    "    questionToken = nltk.word_tokenize(questionstring)\n",
    "    if \"?\" in questionToken:\n",
    "        questionToken.remove(\"?\")\n",
    "    for word in questionToken:\n",
    "        if word.lower() not in stop_words_general:\n",
    "            newq.append(word)\n",
    "    #print (newq)\n",
    "    return newq\n",
    "\n",
    "test1=processq(questionstring1)\n",
    "test2=processq(questionstring2)\n",
    "test3=processq(questionstring3)\n",
    "test4=processq(questionstring4)\n",
    "test5=processq(questionstring5)\n",
    "test6=processq(questionstring6)\n",
    "test7=processq(questionstring7)\n",
    "test8=processq(questionstring8)\n",
    "test9=processq(questionstring9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form a search query for different types of questions\n",
    "## For ceo names\n",
    "it's pretty easy in the way that the query only needs to contain \"ceo\" and the company name\n",
    "a) use a subfunction called get_organization to get the name of the company\n",
    "b) get the query for elastic search\n",
    "## For queries about bankruptcy\n",
    "use the same function get_organization to get the entity, but also need match years and months\n",
    "## GDP query\n",
    "\"GDP\" and keyverbs such as \"affects,effect, affect, effects\"\n",
    "## Specific reason for GDP\n",
    "\"GDP\"and that reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import Search, query\n",
    "import re\n",
    "def get_organization(textlist):\n",
    "    ne_tree = nltk.ne_chunk(nltk.pos_tag(textlist))\n",
    "\n",
    "    named_entities=[]\n",
    "    for t in ne_tree.subtrees():\n",
    "        if t.label() == 'PERSON' or t.label() == 'ORGANIZATION': #as organizations sometimes defined as person\n",
    "            named_entities.append(\" \".join([token for token, pos in t.leaves()]))\n",
    "    return named_entities\n",
    " \n",
    "def get_query_ceo(textlist):\n",
    "    companyname=get_organization(textlist)\n",
    "    companyname.append('CEO')\n",
    "    #print(companyname)\n",
    "    if len(companyname)==0:\n",
    "        print(\"we can't recognize the company name\")\n",
    "\n",
    "    querystring= \" AND \".join(companyname)\n",
    "    queryq=query.Q(\"query_string\",query=querystring)\n",
    "    #print (queryq)\n",
    "    return queryq\n",
    "\n",
    "def get_query_company(textlist):\n",
    "    #need to capture numeric year as well as Month\n",
    "    monthdict={'January':1,'February':2,'March':3,'April':4,'May':5,'June':6,'July':7,'August':8,'September':9,'October':10,'November':11,'December':12}\n",
    "    newmonth=[d for d in textlist if d in monthdict.keys()]\n",
    "    monthval=newmonth[0]\n",
    "    monthnum=monthdict[monthval]\n",
    "    qyear=[]\n",
    "    for item in textlist:\n",
    "        year=re.findall(r\"\\d{4}\",item)\n",
    "        if len(year)>0:\n",
    "            qyear.append(year)\n",
    "    trueyear=int(qyear[0][0])\n",
    "    #print (trueyear)\n",
    "   \n",
    "    q1 = query.Q(\"query_string\", query = \"bankruptcy bankrupt chapter filed declared\" + \" \" +str(trueyear)+\" \"+str(monthval))\n",
    "    return q1,str(trueyear),monthval\n",
    "\n",
    "def get_query_gdp_reason(textlist):\n",
    "    q1 = query.Q(\"query_string\",query=\"GDP\"+\" \"+\"effects effect affect affects\")\n",
    "    #print (q1)\n",
    "    return q1\n",
    "\n",
    "def get_query_gdp_specific_percentage(textlist): #has to be in the list of q2's answer\n",
    "    shouldnotincludelist=['%','percent','percentage','drop','gdp','associated','caused','increase']\n",
    "    q2answerlist=['public debt', 'hurricane', 'credit', 'market', 'cost', 'investment', 'tax', 'deficit', 'oil', 'spend', 'debt']\n",
    "    wordtaglist= nltk.pos_tag(textlist)\n",
    "    askterm=[]\n",
    "    for word,tag in wordtaglist:\n",
    "        if tag=='NN':\n",
    "            if word not in shouldnotincludelist:\n",
    "                askterm.append(word)\n",
    "    #print (askterm)\n",
    "    \n",
    "    askword=' '.join(askterm)\n",
    "    #print (askword)\n",
    "\n",
    "    q1 = query.Q(\"query_string\", query = askword + \" \" +\"GDP\")\n",
    "    #print (q1)\n",
    "    return q1,askword\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get documents searched result\n",
    "Since we are using elasticsearch, the documents already ranked by similarities of keywords. So we chose top # of sentences by chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def search(queryq):\n",
    "\n",
    "def serach_from_query(queryq,chosensize):\n",
    "    s=es.search(index='article_index',q=queryq,size=chosensize)\n",
    "\n",
    "    candidates_docs=[]\n",
    "    for i in range(len(s['hits']['hits'])):\n",
    "        candidates_docs.append(s['hits']['hits'][i]['_source']['body'])\n",
    "\n",
    "    #print (candidates_docs)\n",
    "    return candidates_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer-generated by potential documents\n",
    "Rule based vs. Automated classifier\n",
    "\n",
    "For CEO names, we use simple rule-based method as what we did similarly in hw3\n",
    "select a subset of sentences that contain the word CEO and then choose the most common NER tag(using Spacy) that marks as Person(at least two words length) \n",
    "\n",
    "For Companies that go bankrupt, we select sentences if the sentence contains bankrupt/bankruptcy/chapter, and use spacy to filter if it's an organization or a government entity. Then select the most common word used.\n",
    "\n",
    "For GDP, tf-idf is calculated for keywords and the phrase must both be nouns to be a reason. stopword dictionary is extended to delete uselss words\n",
    "\n",
    "For GDP's reasons, regex is used to capture the percentage. Min val is returned. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_ceo(candidate_docs):\n",
    "    candidatelistofsentences=[]\n",
    "    for doc in candidate_docs:\n",
    "        senttoken=sent_tokenize(doc)\n",
    "        sent_ceo=filter(lambda x: \"ceo\" in x.lower(),senttoken)\n",
    "        candidatelistofsentences.extend(sent_ceo)\n",
    "    person_list=[]\n",
    "    ceo_counts={}\n",
    "    \n",
    "    nlp=spacy.load('en_core_web_sm')\n",
    "    for itemsent in candidatelistofsentences:\n",
    "        doc=nlp(itemsent)\n",
    "        for entity in doc.ents:\n",
    "            if entity.label_ =='PERSON':\n",
    "                if (len(entity.text.split()))>1:\n",
    "                    person_list.append(entity.text)\n",
    "\n",
    "    for item in person_list:\n",
    "        occurence = map(lambda s: item in s, candidatelistofsentences)\n",
    "        ceo_counts[item] = sum(occurence)\n",
    "        ceo_counts=Counter(ceo_counts)\n",
    "        \n",
    "        result=ceo_counts.most_common(1)[0][0]\n",
    "    #print (ceo_counts)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "def get_answer_company(candidate_docs,yearval,monthval):\n",
    "    candidatelistofsentences=[]\n",
    "    bankruptylist=['bankrupt','bankruptcy','chapter']\n",
    "    for doc in candidate_docs:\n",
    "        senttoken=sent_tokenize(doc) #list of sentences\n",
    "        for everysent in senttoken:\n",
    "            if any(word in everysent for word in bankruptylist):\n",
    "                if monthval in everysent:\n",
    "                    if yearval in everysent:\n",
    "                        candidatelistofsentences.append(everysent)\n",
    "    #print(candidatelistofsentences)\n",
    "    company_list=[]\n",
    "    company_counts={}\n",
    "    nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "    for itemsent in candidatelistofsentences:\n",
    "\n",
    "        doc=nlp(itemsent)\n",
    "    \n",
    "        for entity in doc.ents:\n",
    "            if entity.label_ =='ORG' or entity.label_=='GPE': #GOVERNMENT can go bankrupt\n",
    "                company_list.append(entity.text)\n",
    "    #print (company_list)\n",
    "    \n",
    "    new_company_list=[]\n",
    "    for item in company_list:\n",
    "        occurence = map(lambda s: item in s, candidatelistofsentences)\n",
    "        company_counts[item] = sum(occurence)\n",
    "        company_counts=Counter(company_counts)\n",
    "        #print (company_counts)\n",
    "        result=company_counts.most_common(1)[0][0]\n",
    "    return result             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def get_answer_gdp_reason(candidate_docs):\n",
    "    candidatelistofsentences=[]\n",
    "    for doc in candidate_docs:\n",
    "        senttoken=sent_tokenize(doc) #list of sentences\n",
    "        for everysent in senttoken:\n",
    "            if \"GDP\" in everysent:\n",
    "                candidatelistofsentences.append(everysent)\n",
    "    #print (candidatelistofsentences)\n",
    "    #print (len(candidatelistofsentences))\n",
    "    \n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemmatized = [[lmtzr.lemmatize(word,'v') for word in word_tokenize(s)]\n",
    "                  for s in candidatelistofsentences]\n",
    "\n",
    "\n",
    "    cleaned_lemmatizedlist=[]\n",
    "    for everylist in lemmatized:\n",
    "        #print (everylist)\n",
    "        newtrial=' '.join(['%s' % w for w in everylist])\n",
    "        cleaned_lemmatizedlist.append(newtrial)\n",
    "\n",
    "    #print (cleaned_lemmatizedlist)    \n",
    "    tf = TfidfVectorizer(ngram_range = (1, 2),stop_words=stopwordslist)\n",
    "    tfidf_matrix = tf.fit_transform(cleaned_lemmatizedlist)\n",
    "    feature_names=tf.get_feature_names()\n",
    "    \n",
    "    term_freqs = np.sum(tfidf_matrix, axis = 0)\n",
    "    inds = np.argsort(term_freqs)[:, -100:]\n",
    "    words = np.array(tf.get_feature_names())[inds]\n",
    "\n",
    "    #print (words)\n",
    "\n",
    "    flat_word_list = [item for sublist in words for item in sublist]\n",
    "\n",
    "    testnnword=[]\n",
    "    testpostag=nltk.tag.pos_tag(flat_word_list)\n",
    "    for word,tag in testpostag:\n",
    "\n",
    "       # print (word,tag)\n",
    "        if tag =='NN'or tag =='NNS' or tag=='NNP':\n",
    "\n",
    "            if word not in stopwordslist:\n",
    "                testnnword.append(word)\n",
    "    #print (testnnword)\n",
    "    newstopwords=['country','result','data','project','forecast','rate','china','term','world','growth','change','drop','figure','level','fall','report','impact','see','grow','rise','economy','rise','years','cbo','price','government','increase','point','quarter','year','countries','decline']\n",
    "\n",
    "    reasons=[]\n",
    "    for longword in testnnword:\n",
    "        if any(word in longword for word in newstopwords):\n",
    "            continue\n",
    "        else:\n",
    "            reasons.append(longword)\n",
    "    #print (reasons)  \n",
    "    return reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_gdp_pct(candidate_docs,askwordq):\n",
    "    candidatelistofsentences=[]\n",
    "    for doc in candidate_docs:\n",
    "        senttoken=sent_tokenize(doc) #list of sentences\n",
    "        for everysent in senttoken:\n",
    "            if \"GDP\" in everysent:\n",
    "                if askwordq in everysent:\n",
    "                    candidatelistofsentences.append(everysent)\n",
    "    #print (candidatelistofsentences)\n",
    "    #print (len(candidatelistofsentences))\n",
    "    \n",
    "    percents = [ ]\n",
    "    pattern = re.compile(\"\\d+(?:\\.\\d+)?(?:%| percent(?:(?:age|ile)? points)?)\")\n",
    "    for sentence in candidatelistofsentences:\n",
    "        matches = re.findall(pattern, sentence)\n",
    "        percents.extend(matches)\n",
    "    #print (percents)\n",
    "\n",
    "    numericvallist=[]\n",
    "    pattern2=re.compile(\"\\d+(?:\\.\\d+)?\")\n",
    "    for percent in percents:\n",
    "        matchespct = re.findall(pattern2, percent)\n",
    "        numericvallist.extend(matchespct)\n",
    "    \n",
    "    #print (min(numericvallist))\n",
    "    answerstring = min(numericvallist)+'%'\n",
    "    #print (answerstring)\n",
    "    return answerstring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning:\n",
    "The following two cells is for reference of how to get gdp's reason. I used a formal version of it as a function, but it is left here as referencing the above result I got "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244\n",
      "[['public debt' 'private' 'hurricane' 'come' 'actual' 'trillion' 'shrink'\n",
      "  'credit' '30' 'however' '2023' 'continue' 'second' 'higher'\n",
      "  'government debt' 'country' 'q1' 'data' 'reduce' 'chart' 'china'\n",
      "  'remain' '100' 'also' 'end' 'result' 'market' 'project' 'per'\n",
      "  'forecast' 'rate' 'world' 'around' 'total' 'real growth' '2012' '2008'\n",
      "  'annual' '2007' 'change' 'drop' 'term' 'federal' 'cost' 'since'\n",
      "  'public' 'figure' 'fed' 'expect' 'global' 'economic growth' 'lower'\n",
      "  'potential' 'cbo' 'billion' 'negative' 'level' 'find' 'investment'\n",
      "  'first' 'affect' 'fall' 'report' 'oil price' 'much' 'impact' 'could'\n",
      "  'tax' 'average growth' 'see' 'deficit' 'add' 'grow' 'say' 'current'\n",
      "  'decline' 'rise' '2013' '90' 'us' '2014' 'countries' 'show' 'economy'\n",
      "  'years' 'price' 'government' 'increase' 'oil' 'point' 'spend' 'real'\n",
      "  'economic' 'quarter' 'would' 'average' 'year' 'estimate' 'debt'\n",
      "  'growth']]\n",
      "['public debt', 'hurricane', 'credit', 'government debt', 'country', 'data', 'china', 'result', 'market', 'project', 'forecast', 'rate', 'world', 'real growth', 'change', 'drop', 'term', 'cost', 'figure', 'economic growth', 'cbo', 'level', 'investment', 'fall', 'report', 'impact', 'tax', 'see', 'deficit', 'grow', 'decline', 'rise', 'countries', 'economy', 'years', 'price', 'government', 'increase', 'oil', 'point', 'spend', 'quarter', 'year', 'debt', 'growth']\n",
      "['public debt', 'hurricane', 'credit', 'market', 'cost', 'investment', 'tax', 'deficit', 'oil', 'spend', 'debt']\n"
     ]
    }
   ],
   "source": [
    "def get_answer_gdp_reason_test(candidate_docs):\n",
    "    candidatelistofsentences=[]\n",
    "    for doc in candidate_docs:\n",
    "        senttoken=sent_tokenize(doc) #list of sentences\n",
    "        for everysent in senttoken:\n",
    "            if \"GDP\" in everysent:\n",
    "                candidatelistofsentences.append(everysent)\n",
    "    #print (candidatelistofsentences)\n",
    "    print (len(candidatelistofsentences))\n",
    "    return candidatelistofsentences\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "sentsgdp=get_answer_gdp_reason_test(gdpdoc)\n",
    "#print (sentsgdp)\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "lemmatized = [[lmtzr.lemmatize(word,'v') for word in word_tokenize(s)]\n",
    "              for s in sentsgdp]\n",
    "\n",
    "\n",
    "cleaned_lemmatizedlist=[]\n",
    "for everylist in lemmatized:\n",
    "    #print (everylist)\n",
    "    newtrial=' '.join(['%s' % w for w in everylist])\n",
    "    cleaned_lemmatizedlist.append(newtrial)\n",
    "    \n",
    "#print (cleaned_lemmatizedlist)    \n",
    "tf = TfidfVectorizer(ngram_range = (1, 2),stop_words=stopwordslist)\n",
    "tfidf_matrix = tf.fit_transform(cleaned_lemmatizedlist)\n",
    "feature_names=tf.get_feature_names()\n",
    "\n",
    "term_freqs = np.sum(tfidf_matrix, axis = 0)\n",
    "inds = np.argsort(term_freqs)[:, -100:]\n",
    "words = np.array(tf.get_feature_names())[inds]\n",
    "\n",
    "print (words)\n",
    "\n",
    "flat_word_list = [item for sublist in words for item in sublist]\n",
    "\n",
    "testnnword=[]\n",
    "testpostag=nltk.tag.pos_tag(flat_word_list)\n",
    "for word,tag in testpostag:\n",
    "    \n",
    "   # print (word,tag)\n",
    "    if tag =='NN'or tag =='NNS' or tag=='NNP':\n",
    "\n",
    "        if word not in stopwordslist:\n",
    "            testnnword.append(word)\n",
    "print (testnnword)\n",
    "newstopwords=['country','result','data','project','forecast','rate','china','term','world','growth','change','drop','figure','level','fall','report','impact','see','grow','rise','economy','rise','years','cbo','price','government','increase','point','quarter','year','countries','decline']\n",
    "\n",
    "reasons=[]\n",
    "for longword in testnnword:\n",
    "    if any(word in longword for word in newstopwords):\n",
    "        continue\n",
    "    else:\n",
    "        reasons.append(longword)\n",
    "print (reasons)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.6%', '4.8%', '40%', '1 percent', '10%', '0.2%', '20%', '1%', '1.6%', '4.8%', '1 percent', '0.4 percentage points', '0.2 percent', '35%', '90%', '20%', '0.5%', '1.3%', '9%', '12.7%', '10%', '4.7%', '8.0%', '20%', '0.5%', '1.3%', '9%', '12.7%', '10%', '4.7%', '8.0%', '1.6%', '1.6%', '1.5%', '1.5%', '0.2%', '0.5%']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.2%'"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "querytestgdpperct,askwordq=get_query_gdp_specific_percentage(test9)\n",
    "perctdoc=serach_from_query(querytestgdpperct,50)\n",
    "get_answer_gdp_pct(perctdoc,askwordq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_the_question(questionstring):\n",
    "    typeq= classify_questions(questionstring)\n",
    "    questiontextlist=processq(questionstring)\n",
    "    if typeq==1:\n",
    "        company_query,yearval,monthval=get_query_company(questiontextlist)\n",
    "        candidate_doc=serach_from_query(company_query,1000)\n",
    "        names=get_answer_company(candidate_doc,yearval,yearval)\n",
    "    if typeq==2:\n",
    "        querytestgdp=get_query_gdp_reason(questiontextlist)\n",
    "        gdpdoc=serach_from_query(querytestgdp,100)\n",
    "        names=get_answer_gdp_reason(gdpdoc)\n",
    "    if typeq==3:\n",
    "        querytestgdpperct,askwordq=get_query_gdp_specific_percentage(questiontextlist)\n",
    "        perctdoc=serach_from_query(querytestgdpperct,10)\n",
    "        names=get_answer_gdp_pct(perctdoc,askwordq)\n",
    "    if typeq==4:\n",
    "        ceo_query=get_query_ceo(questiontextlist)\n",
    "        candidate_doc=serach_from_query(ceo_query,50)\n",
    "        names=get_answer_ceo(candidate_doc) #tesla - elon musk\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['public debt', 'hurricane', 'credit', 'market', 'cost', 'investment', 'tax', 'deficit', 'oil', 'spend', 'debt']\n",
      "['public debt', 'hurricane', 'credit', 'market', 'cost', 'investment', 'tax', 'deficit', 'oil', 'spend', 'debt']\n",
      "['public debt', 'hurricane', 'credit', 'market', 'cost', 'investment', 'tax', 'deficit', 'oil', 'spend', 'debt']\n"
     ]
    }
   ],
   "source": [
    "print(answer_the_question(\"what impacts GDP\"))\n",
    "print(answer_the_question(\"what affects GDP\"))\n",
    "print(answer_the_question(\"what causes GDP\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2%\n",
      "0.8%\n",
      "1.18%\n"
     ]
    }
   ],
   "source": [
    "print(answer_the_question(\"what percent of increase in gdp is associated with oil price\"))\n",
    "print(answer_the_question(\"what percentage of drop in gdp is associated with tax\"))\n",
    "print(answer_the_question(\"what % of drop in gdp is caused by investment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larry Page\n",
      "Mark Zuckerberg\n",
      "Ginni Rometty\n",
      "James Gorman\n"
     ]
    }
   ],
   "source": [
    "print (answer_the_question('who is the CEO of company Google?'))\n",
    "print(answer_the_question('who is the CEO of Facebook'))\n",
    "print(answer_the_question('who is the ceo of the company IBM'))\n",
    "print(answer_the_question('who is the ceo of the company Morgan Stanley'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detroit\n",
      "Lehman\n",
      "GM\n"
     ]
    }
   ],
   "source": [
    "print(answer_the_question('which comapany went bankrupt in July of 2013?'))\n",
    "print(answer_the_question(\"Which company went bankrupt in September 2008?\"))\n",
    "print(answer_the_question(\"which company declared bankruptcy in June 2009?\"))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
